# -*- coding: utf-8 -*-
"""Entregable_1_2_y_3_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvsauFKpbhQPATYTpcXmtvnuFe2NAduO
"""

from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import os
import pandas as pd
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
from nltk.tokenize import sent_tokenize

from google.colab import drive
drive.mount('/content/drive')

# prompt: help me to set the HF_TOKEN environment variable and do login to hugging face

from huggingface_hub import notebook_login
notebook_login()

"""# Entregable 1: Clasificar las entidades nombradas de las historias clinicas"""

model = AutoModelForTokenClassification.from_pretrained(
    "anvorja/breast-cancer-biomedical-ner-sp"
)
tokenizer = AutoTokenizer.from_pretrained("anvorja/breast-cancer-biomedical-ner-sp")

ner_pipeline = pipeline("token-classification", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

import os
import pandas as pd
from nltk.tokenize import sent_tokenize   # or the splitter you already use


def procesar_historias_clinicas(path):
    """
    Re-implemented so that consecutive entities of the same type (except FECHA)
    are fused into a single row in the output DataFrame.
    """
    datos = []
    archivos = [f for f in os.listdir(path) if f.endswith(".txt")]
    total_archivos = len(archivos)
    print(f"\nSe encontraron {total_archivos} archivos de historias clÃ­nicas.\n")

    for idx, archivo in enumerate(archivos, 1):
        print(f"Procesando archivo {idx}/{total_archivos}: {archivo}")
        ruta_archivo = os.path.join(path, archivo)

        with open(ruta_archivo, encoding='utf-8') as f:
            texto = f.read()

        for i, oracion in enumerate(sent_tokenize(texto)):
            entidades = ner_pipeline(oracion)

            for ent in entidades:
                datos.append({
                    "patient_id": archivo.replace(".txt", ""),
                    "sentence_id": i,
                    "sentence": oracion,
                    "NER": ent["word"],
                    "label": ent["entity_group"],
                    "start": ent["start"],
                    "end": ent["end"]
                })

    return pd.DataFrame(datos)

df_ner = procesar_historias_clinicas("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/Notas_Cancer_Mama/")

df_ner.head(26)

df_ner.to_csv("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/entidades_extraidas_4_anvorja_model_entregable_1.csv")

"""# Entregable 2: Clasificar la negaciÃ³n o la incertidumbre de las oraciones de las historias"""

labels = ['B-NEG', 'B-NSCO', 'B-UNC', 'B-USCO', 'I-NEG', 'I-NSCO', 'I-UNC', 'I-USCO', 'O']

id2label = {k: v for k, v in enumerate(labels)}
label2id = {v: k for k, v in enumerate(labels)}

neg_model = "JuanSolarte99/bert-base-uncased-finetuned-ner-negation_detection_NUBES"
model = AutoModelForTokenClassification.from_pretrained(
    neg_model,
    id2label=id2label,
    label2id=label2id
)
tokenizer = AutoTokenizer.from_pretrained(neg_model)

neg_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

import os
import pandas as pd
from nltk.tokenize import sent_tokenize   # or the splitter you already use


def procesar_historias_clinicas_negacion(path):
    """
    Re-implemented so that consecutive entities of the same type (except FECHA)
    are fused into a single row in the output DataFrame.
    """
    datos = []
    archivos = [f for f in os.listdir(path) if f.endswith(".txt")]
    total_archivos = len(archivos)
    print(f"\nSe encontraron {total_archivos} archivos de historias clÃ­nicas.\n")

    for idx, archivo in enumerate(archivos, 1):
        print(f"Procesando archivo {idx}/{total_archivos}: {archivo}")
        ruta_archivo = os.path.join(path, archivo)

        with open(ruta_archivo, encoding='utf-8') as f:
            texto = f.read()

        for i, oracion in enumerate(sent_tokenize(texto)):
            entidades = neg_pipeline(oracion)

            for ent in entidades:
                datos.append({
                    "patient_id": archivo.replace(".txt", ""),
                    "sentence_id": i,
                    "sentence": oracion,
                    "NER": ent["word"],
                    "label": ent["entity_group"],
                    "start": ent["start"],
                    "end": ent["end"]
                })

    return pd.DataFrame(datos)

df_ner_negacion = procesar_historias_clinicas_negacion("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/Notas_Cancer_Mama/")

df_ner_negacion.head(10)

import pandas as pd
import json

def build_sentence_level_df(df_tokens: pd.DataFrame) -> pd.DataFrame:
    """
    Parameters
    ----------
    df_tokens : DataFrame
        Columns required:
        â”€ patient_id, sentence_id, sentence, start, end, label

    Returns
    -------
    DataFrame
        One row per (patient_id, sentence_id, sentence) with:
        â€¢ spans : str   (JSON list of merged spans)
        â€¢ sentence_classified : str  (sentence with [TAG] in-line)
    """

    rows = []
    group_cols = ['patient_id', 'sentence_id', 'sentence']

    for (pid, sid, sent), g in df_tokens.groupby(group_cols, sort=False):
        # ---- 1. take spans for *this* sentence ---------------------------
        spans = (g[['start', 'end', 'label']]
                 .sort_values('start')
                 .query("label != 'O'")          # keep, if you use 'O'
                 .to_dict('records'))

        # ---- 2. merge adjacent sub-tokens with same label ----------------
        merged = []
        for sp in spans:
            if merged and sp['label'] == merged[-1]['label'] and sp['start'] == merged[-1]['end']:
                merged[-1]['end'] = sp['end']    # extend previous span
            else:
                merged.append(sp.copy())

        # ---- 3. build annotated sentence --------------------------------
        pieces, cursor = [], 0
        for sp in merged:
            if sp['start'] < cursor:             # overlap â†’ skip
                continue
            pieces.append(sent[cursor:sp['start']])
            pieces.append((f"({sent[sp['start']:sp['end']]})"))
            pieces.append(f"[{sp['label']}]")
            cursor = sp['end']
        pieces.append(sent[cursor:])
        sentence_classified = ''.join(pieces)

        # ---- 4. collect row ---------------------------------------------
        rows.append({
            'patient_id': pid,
            'sentence_id': sid,
            'sentence': sent,
            'spans': json.dumps(merged, ensure_ascii=False),
            'sentence_classified': sentence_classified
        })

    return pd.DataFrame(rows)

# ------------------------------------------------------------------------
# USO:
df_sentences = build_sentence_level_df(df_ner_negacion)

# show entire column contents (no truncation)
pd.set_option('display.max_colwidth', None)
df_sentences[['sentence_classified']].head(10)

df_sentences.to_csv("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/negaciones_extraidas_entregable_2.csv")

"""# Entregable 3: Integrar los modelos anteriores (NER, NegaciÃ³n) en un sript que permita procesar las historias clÃ­nicas y producir una base de datos estructurada."""

df = pd.read_csv("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/entidades_extraidas_4_anvorja_model_entregable_1.csv")

# Agregar columna de estado
df["Estado"] = None  # Por defecto

# Agrupar por patient_id para mostrar el progreso
pacientes = df["patient_id"].unique()
print(f"\nðŸ“„ Se encontraron {len(pacientes)} pacientes para anÃ¡lisis de negaciÃ³n.\n")

# Procesar por paciente y oraciÃ³n
for i, pid in enumerate(pacientes, 1):
    df_paciente = df[df["patient_id"] == pid]
    oraciones_unicas = df_paciente["sentence"].unique()

    print(f"ðŸ‘¤ Procesando paciente {i}/{len(pacientes)}: {pid} ({len(oraciones_unicas)} oraciones)")

    for j, oracion in enumerate(oraciones_unicas, 1):
        # Mostrar progreso de oraciÃ³n
        print(f"   â†ª OraciÃ³n {j}/{len(oraciones_unicas)}")

        #predicciones = neg_pipeline(oracion)

        # Obtener entidades en esa oraciÃ³n
        entidades = df[(df["patient_id"] == pid) & (df["sentence"] == oracion)]

        # Buscar si alguna entidad cae dentro del texto anotado
        for idx, fila in entidades.iterrows():
            # Pasar al modelo
            predicciones = neg_pipeline(str(fila["NER"]))

            if len(predicciones) > 1:
                tipos = [pred["entity_group"] for pred in predicciones]
                if ("NEG" in tipos) or ("NSCO" in tipos):
                    df.at[idx, "Estado"] = "NEGATIVE"
                elif ("UNC" in tipos) or ("USCO" in tipos):
                    df.at[idx, "Estado"] = "UNCERTAIN"
                continue

            if len(predicciones) == 0:
                df.at[idx, "Estado"] = "POSITIVE"
                continue

            pred = predicciones[0]

            if pred["entity_group"] in ("NEG", "NSCO"):
                df.at[idx, "Estado"] = "NEGATIVE"
            elif pred["entity_group"] in ("UNC", "USCO"):
                df.at[idx, "Estado"] = "UNCERTAIN"
            else:
                df.at[idx, "Estado"] = "POSITIVE"

df.head(26)

df.to_csv("/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_2/entidades_con_estado_entregable_3.csv")