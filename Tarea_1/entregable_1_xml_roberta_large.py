# -*- coding: utf-8 -*-
"""Entregable_1_xml_roberta_large.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15WihkAe7CBKsneC6b1X4vG0qSm8u440w

#Proceso de Fine-Tuning para Extracción de Conceptos Medicos relacionados con el Cancer de Pulmon

### Primero se define la ruta a los archivos del corpus anotado de cancer de pulmon
"""

from google.colab import drive
drive.mount('/content/drive')

ruta_corpus = '/content/drive/MyDrive/Tareas_Analitica_Datos_Salud/Tarea_1/lung_cancer_corpus/'

# prompt: I want to print files available on the route declared in ruta_corpus

import os

# List files in the specified directory
for filename in os.listdir(ruta_corpus):
  print(filename)

"""### Luego aseguramos de tener instaladas las librería necesarias para hacer el entrenamiento fino del modelo"""

!pip install datasets transformers
!pip install seqeval
!pip install -U datasets evaluate
!pip install -U huggingface_hub

"""### Cargamos el corpus y alistamos los datos de entrenamiento, testeo y validacion"""

from datasets import DatasetDict, Dataset, Features, Sequence, Value, ClassLabel
from collections import defaultdict
import pandas as pd
from pathlib import Path

def leer_archivo_corpus(ruta_archivo):
    """Lee un archivo csv y devuelve un diccionario con tokens y etiquetas agrupados por sentencia."""
    datos = defaultdict(list)

    # Leer el archivo CSV usando pandas
    df = pd.read_csv(ruta_archivo)

    # Asegurar que Word y Tag sean strings
    df['Word'] = df['Word'].astype(str)
    df['Tag'] = df['Tag'].astype(str)

    # Rellenar valores faltantes en Sentence # con el último valor válido
    df['Sentence #'] = df['Sentence #'].ffill()

    # Agrupar por número de sentencia
    for _, grupo in df.groupby('Sentence #'):
        # Extraer tokens y etiquetas de la sentencia actual
        tokens_sentencia = grupo['Word'].tolist()
        labels_sentencia = grupo['Tag'].tolist()

        # Agregar los tokens y labels de la sentencia actual
        datos["tokens"].append(tokens_sentencia)
        datos["ner_tags"].append(labels_sentencia)

    return datos


def cargar_datasets(rutas_archivos):
    """Carga archivos .bio y devuelve un DatasetDict."""
    datasets = {}
    for nombre, ruta in rutas_archivos.items():
        datos = leer_archivo_corpus(ruta)
        print(f"Dataset {nombre} cargado correctamente.")
        print(f"Tamaño del dataset {nombre}: {len(datos['tokens'])}")
        print(f"Tokens del dataset {nombre}: {datos['tokens'][:5]}")
        print(f"Etiquetas del dataset {nombre}: {datos['ner_tags'][:5]}")
        datasets[nombre] = Dataset.from_dict(datos)

    return DatasetDict(datasets)

def detectar_etiquetas_unicas(rutas_archivos):
    """Detecta automáticamente todas las etiquetas únicas en los archivos."""
    todas_etiquetas = set()

    for ruta in rutas_archivos.values():
      df = pd.read_csv(ruta)
      etiquetas = df['Tag'].unique()
      todas_etiquetas.update(etiquetas)

    # Ordenamos las etiquetas para que 'O' sea la última
    etiquetas_ordenadas = sorted(todas_etiquetas - {'O'}) + ['O']
    return etiquetas_ordenadas

rutas_archivos = {
    "train": ruta_corpus + "sentences_train.csv",
    "test":  ruta_corpus + "sentences_test.csv",
    "valid": ruta_corpus + "sentences_dev.csv"
}

# Detectar automáticamente todas las etiquetas
LABELS = detectar_etiquetas_unicas(rutas_archivos)
print("Etiquetas detectadas:", LABELS)

# Cargar los datasets
dataset_dict = cargar_datasets(rutas_archivos)

# Definir la estructura de features con las etiquetas detectadas
features = Features({
    "tokens": Sequence(Value("string")),
    "ner_tags": Sequence(ClassLabel(names=LABELS))
})

# Aplicar el casting a cada split
for split in dataset_dict:
    dataset_dict[split] = dataset_dict[split].cast(features)

# Mostrar información del dataset
print("\nDataset cargado correctamente:")
print(dataset_dict)

# Mostrar un ejemplo del conjunto de entrenamiento
print("\nEjemplo del train:")
print(dataset_dict["train"][0])

# Mostrar las características del dataset
print("\nCaracterísticas del dataset:")
print(dataset_dict["train"].features)

"""### Iniamos el proceso de Fine-Tuning"""

task = 'ner'
x = dataset_dict["train"].features[f"{task}_tags"].feature.names
print(x)

from huggingface_hub import login

# token cuenta personal, maestria_laptop_james_lectura
maestria_laptop_james_lectura = '___TOKEN___PRUEBA____'
colab_push_token = '___TOKEN___PRUEBA____'
login(maestria_laptop_james_lectura)

task = "ner" # Should be one of "ner", "pos" or "chunk"
model_checkpoint = "xlm-roberta-large"
batch_size = 8

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_datasets = dataset_dict.map(
    tokenize_and_align_labels,
    batched=True
)

label_list = dataset_dict["train"].features[f"{task}_tags"].feature.names
label_list

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

# Create id2label and label2id mappings
id2label = {i: label for i, label in enumerate(label_list)}
label2id = {label: i for i, label in enumerate(label_list)}

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)

model_bert_base = model_checkpoint.split("/")[-1]
args = TrainingArguments(
    f"{model_bert_base}-finetuned-{task}-lung-cancer",
    eval_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=4, # Reduced number of epochs
    weight_decay=0.01,
    push_to_hub=True,
    hub_token=colab_push_token, # hub token para escritura
)

"""### Evaluación del entrenamiento"""

from evaluate import load
metric = load("seqeval")

import numpy as np

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

test_metrics = trainer.evaluate(tokenized_datasets["test"])
print("\n" + "="*50)
print(f"Resultados finales en conjunto de test:")
print(f"F1-score: {test_metrics['eval_f1']:.3f}")
print(f"Precisión: {test_metrics['eval_precision']:.3f}")
print(f"Recall: {test_metrics['eval_recall']:.3f}")
print("="*50)

trainer.push_to_hub()

label_names =  dataset_dict["train"].features["ner_tags"].feature.names
label_names

predictions, labels, _ = trainer.predict(tokenized_datasets["test"])
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
results

